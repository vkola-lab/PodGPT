# Database evaluation
Evaluate PodGPT, PodGPT with Retrieval-augmented Generation (RAG), or ChatGPT on your own database!

## ğŸ’» Installation
In addition to the [**PodGPT dependencies**](https://github.com/vkola-lab/PodGPT/blob/main/requirements.txt), please make sure to install the following packages:
```bash
pip install pgvector
pip install flask-sqlalchemy
pip install psycopg2
```

## ğŸ“– Prepare your own database
We used the MMLU Professional medicine database as an example here:
<p align="center">
  <a href="https://www.medrxiv.org/content/10.1101/2024.07.11.24310304v2"> <img src="figures/database.png"></a> 
</p>
To prepare your own database, please make sure you have:

1. `question`: the question is in the first column (`column A`) of our demo database.
2. `options`: we have four options (A, B, C, and D) in our demo database, from `column B` to `column E`.
3. `answer`: The ground truth answer is located in the fifth column (`column F`) of our demo database.

## ğŸš€ Inference and benchmarking
```bash
python main.py --mode podgpt --rag True --eval_pretrain True
```
1. `--mode`: Evaluate PodGPT or ChatGPT: podgpt/chatgpt. The default is "podgpt".
2. `--rag`: Whether to use RAG database and pipeline: True/False. The default is True.
2. `--eval_pretrain`: Evaluate the original pre-trained model: True/False. The default is True.

## ğŸï¸ Structure of the code
At the root of this folder, you will see:
```text
â”œâ”€â”€ main.py
â”œâ”€â”€ config_podgpt.yml
â”œâ”€â”€ config_benchmark.yml
â”œâ”€â”€ config_chatgpt.yml
â”œâ”€â”€ lib
â”‚Â Â  â”œâ”€â”€ config.py
â”‚Â Â  â”œâ”€â”€ database.py
â”‚Â Â  â”œâ”€â”€ evaluation.py
â”‚Â Â  â”œâ”€â”€ model_loader.py
â”‚Â Â  â””â”€â”€ pipeline.py
â”œâ”€â”€ benchmark
â”‚Â Â  â””â”€â”€ database.csv
â””â”€â”€ utils
    â”œâ”€â”€ answer_utils.py
    â”œâ”€â”€ benchmark_utils.py
    â”œâ”€â”€ eval_utils.py
    â”œâ”€â”€ utils.py
    â””â”€â”€ vllm_utils.py
```
