<h1 align="center">MedPodGPT</h1>
<h4 align="center">Benchmarking Multilingual Medical Large Language Models (LLMs)</h4>
<p align="center">
  <a href="https://github.com/vkola-lab/medpodgpt"> <img width="250px" src="MedPodGPT.png"></a> 
  <br />
  <br />
  <a href="https://img.shields.io/badge/Code%20License-MIT-green.svg"><img alt="CODE_LICENSE" src="https://img.shields.io/badge/Code%20License-MIT-green.svg" /></a>
  <a href="https://img.shields.io/badge/Data%20License-CC%20By%20NC%204.0-red.svg"><img alt="DATA_LICENSE" src="https://img.shields.io/badge/Data%20License-CC%20By%20NC%204.0-red.svg" /></a>
  <a href="https://www.apache.org/licenses/LICENSE-2.0"><img alt="Model Weight License" src="https://img.shields.io/badge/Model%20Weights%20License-Apache-yellow" /></a>
  <a href="https://www.python.org/downloads/release/python-3100/"><img alt="Python 3.10" src="https://img.shields.io/badge/python-3.10-blue.svg" /></a>
</p>

# ðŸŽ‰ Announcements
[2024.7.12] ![](news.gif) Our AI Platform [MedPodGPT](https://medpodgpt.org/) is available to authorized users. 
It is an online platform for deploying our latest multimodal foundation models for medical and clinical applications.
Please try it out if you are interested!

# ðŸ“š Table of Contents
- [Installation](#Installation)
- [Quick Start](#Quick-Start)
  - [Train Lightweight Models](#Train-Lightweight-Models)
  - [Train Heavy Models](#Train-Heavy-Models)
  - [Train Quantized Models](#Train-Quantized-Models)
- [Performance Evaluation](#Performance-Evaluation)
  - [Single GPU For Lightweight Models](#Single-GPU-For-Lightweight-Models)
  - [Distributed GPUs For Heavy Models](#Distributed-GPUs-For-Heavy-Models)
- [Automatic Speech Recognition](#Automatic-Speech-Recognition)
- [Dataset Builder](#Dataset-Builder)
- [Upload and Download Models](#Upload-and-Download-Models)
- [Structure of the Code](#Structure-of-the-Code)
- [Citation](#Citation)
- [Contact](#Contact)
- [Contribution](#Contribution)
- [Acknowledgement](#Acknowledgement)

# Installation
```shell
pip install -r requirements.txt
```

# Quick Start
## Train Lightweight Models
For lightweight models (2B, 7B, and 8B), we optimize the entire model. 
Please check and setup hyper-parameters in [config_small.yml](https://github.com/vkola-lab/medpodgpt/blob/main/config_small.yml).
```shell
python main_small.py
```

## Train Heavy Models
For lager and heavy models (>8B), we optimize the Low-rank Adapter (LoRA).
Please check and setup hyper-parameters in [config_small.yml](https://github.com/vkola-lab/medpodgpt/blob/main/config_large.yml).
```shell
python main_large.py
```

## Train Quantized Models
We also provide support for quantizing larger models using the [GPTQ](https://arxiv.org/abs/2210.17323) algorithm and then optimizing the LoRA.
The large models can be deployed on consumer GPUs after quantization.
```shell
python quantization.py "./save_folder" "./gptq_model" "medical" --bits 4 --group_size 128 --desc_act 1 --dtype float16
```
Then, we need to upload the model to Hugging Face,
```shell
python upload_quantized_model.py --repo "shuyuej/MedLLaMA3-70B-BASE-MODEL-QUANT" --folder_path "./gptq_model"
```
Lastly, we optimize the LoRA module,
```shell
python main_quantization.py
```

# Performance Evaluation
we use `inference_pretrain.py` and `inference_single_model.py` for larger models (> 8B) 
and `inference_sequential.py` for smaller models (2B/7B/8B).

First, in the project home directory, please copy and paste the inference files,
```shell
cp -r ./inference/inference_sequential.py ./
cp -r ./inference/inference_large.sh ./
cp -r ./inference/inference_pretrain.py ./
cp -r ./inference/inference_single_model.py ./
```

### Single GPU For Lightweight Models
#### inference_sequential.py
**Sequentially** evaluate the performance of multiple checkpoints (models).<br>
Please note that we use `--eval_pretrain` to indicate whether to evaluate the original pre-trained model.
```shell
python inference_sequential.py --eval_pretrain True --id 35166 52749 70332 87915
```

### Distributed GPUs For Heavy Models
**Sequentially** evaluate the performance of the original pre-trained model and all the checkpoints.<br>
Special Notice: Please change the checkpoint IDs and CUDA_VISIBLE_DEVICES in the `inference_large.sh` file.
```shell
sh inference_large.sh
```

#### inference_pretrain.py
**Only** evaluate the performance of the original pre-trained model.
```shell
python inference_pretrain.py
```

#### inference_single_model.py
**Only** evaluate the performance of a single checkpoint (model).<br>
Please note that `--id` is the checkpoint id.
```shell
python inference_single_model.py --id 35166
```

# Automatic Speech Recognition
In the [scripts folder](https://github.com/vkola-lab/medpodgpt/tree/main/scripts), 
we provide Automatic Speech Recognition (ASR) support.
```shell
python audio2text.py
```

# Dataset Builder
We used the following codes to pre-process our transcripts and generate training dataset.
Please check [these lines](https://github.com/vkola-lab/medpodgpt/blob/main/scripts/database_builder.py#L236-L242) for different languages support. 
```shell
python database_builder.py
```
```shell
python merge_database.py
```

# Upload and Download Models
In the [scripts folder](https://github.com/vkola-lab/medpodgpt/tree/main/scripts), 
we offer support for both uploading and downloading models.

To upload your checkpoints to your Hugging Face model repo,
```shell
python upload_model.py --repo "shuyuej/DrGemma2B" --id 35166 52749 70332 87915
```

To download your model or files from Hugging Face,
```shell
python download_model.py --repo "shuyuej/DrGemma2B" --repo_type "model" --save_dir "./save_folder"
```

# Structure of the Code
At the root of the project, you will see:

```text
â”œâ”€â”€ config_chatgpt.yml
â”œâ”€â”€ config_large.yml
â”œâ”€â”€ config_quantization.yml
â”œâ”€â”€ config_small.yml
â”œâ”€â”€ main_large.py
â”œâ”€â”€ main_quantization.py
â”œâ”€â”€ main_small.py
â”œâ”€â”€ lib
â”‚Â Â  â”œâ”€â”€ README.md
â”‚Â Â  â”œâ”€â”€ data_manager.py
â”‚Â Â  â”œâ”€â”€ evaluation_chatgpt.py
â”‚Â Â  â”œâ”€â”€ evaluation_large.py
â”‚Â Â  â”œâ”€â”€ evaluation_small.py
â”‚Â Â  â”œâ”€â”€ model_loader_large.py
â”‚Â Â  â”œâ”€â”€ model_loader_quantization.py
â”‚Â Â  â””â”€â”€ model_loader_small.py
â”œâ”€â”€ inference
â”‚Â Â  â”œâ”€â”€ README.md
â”‚Â Â  â”œâ”€â”€ inference_chatgpt.py
â”‚Â Â  â”œâ”€â”€ inference_large.sh
â”‚Â Â  â”œâ”€â”€ inference_pretrain.py
â”‚Â Â  â”œâ”€â”€ inference_sequential.py
â”‚Â Â  â””â”€â”€ inference_single_model.py
â”œâ”€â”€ download_files
â”‚Â Â  â”œâ”€â”€ README.md
â”‚Â Â  â”œâ”€â”€ download_model_from_hf.py
â”‚Â Â  â””â”€â”€ download_model_to_local.py
â”œâ”€â”€ quantization
â”‚Â Â  â”œâ”€â”€ README.md
â”‚Â Â  â”œâ”€â”€ quantization.py
â”‚Â Â  â””â”€â”€ upload_quantization_model.py
â”œâ”€â”€ scripts
â”‚Â Â  â”œâ”€â”€ README.md
â”‚Â Â  â”œâ”€â”€ audio2text.py
â”‚Â Â  â”œâ”€â”€ database_builder.py
â”‚Â Â  â”œâ”€â”€ download_model.py
â”‚Â Â  â””â”€â”€ merge_database.py
â”œâ”€â”€ setup.cfg
â”œâ”€â”€ upload_model.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ benchmark
â”‚Â Â  â”œâ”€â”€ README.md
â”‚Â Â  â”œâ”€â”€ chinese_cmmlu
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ anatomy.csv
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ clinical_knowledge.csv
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ college_medicine.csv
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ genetics.csv
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ nutrition.csv
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ traditional_chinese_medicine.csv
â”‚Â Â  â”‚Â Â  â””â”€â”€ virology.csv
â”‚Â Â  â”œâ”€â”€ chinese_mcmle
â”‚Â Â  â”‚Â Â  â””â”€â”€ MedQA-MCMLE.jsonl
â”‚Â Â  â”œâ”€â”€ english_medexpqa
â”‚Â Â  â”‚Â Â  â””â”€â”€ test.en.casimedicos.rag.jsonl
â”‚Â Â  â”œâ”€â”€ english_medmcqa
â”‚Â Â  â”‚Â Â  â””â”€â”€ MedMCQA_test.json
â”‚Â Â  â”œâ”€â”€ english_medqa
â”‚Â Â  â”‚Â Â  â””â”€â”€ MedQA_USMLE_test.jsonl
â”‚Â Â  â”œâ”€â”€ english_mmlu
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ anatomy_test.csv
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ clinical_knowledge_test.csv
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ college_biology_test.csv
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ college_medicine_test.csv
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ medical_genetics_test.csv
â”‚Â Â  â”‚Â Â  â””â”€â”€ professional_medicine_test.csv
â”‚Â Â  â”œâ”€â”€ english_pubmedqa
â”‚Â Â  â”‚Â Â  â””â”€â”€ PubMedQA_test.json
â”‚Â Â  â”œâ”€â”€ english_usmle
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ USMLE_STEP_1.json
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ USMLE_STEP_2.json
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ USMLE_STEP_3.json
â”‚Â Â  â”‚Â Â  â””â”€â”€ USMLE_ethics.json
â”‚Â Â  â”œâ”€â”€ french_medexpqa
â”‚Â Â  â”‚Â Â  â””â”€â”€ test.fr.casimedicos.rag.jsonl
â”‚Â Â  â”œâ”€â”€ french_medmcqa
â”‚Â Â  â”‚Â Â  â””â”€â”€ FrenchMedMCQA-test.json
â”‚Â Â  â”œâ”€â”€ french_mmlu
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ mmlu_French_test_anatomy_test.csv
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ mmlu_French_test_clinical_knowledge_test.csv
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ mmlu_French_test_college_biology_test.csv
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ mmlu_French_test_college_medicine_test.csv
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ mmlu_French_test_medical_genetics_test.csv
â”‚Â Â  â”‚Â Â  â””â”€â”€ mmlu_French_test_professional_medicine_test.csv
â”‚Â Â  â”œâ”€â”€ hindi_mmlu
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ mmlu_Hindi_test_anatomy_test.csv
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ mmlu_Hindi_test_clinical_knowledge_test.csv
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ mmlu_Hindi_test_college_biology_test.csv
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ mmlu_Hindi_test_college_medicine_test.csv
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ mmlu_Hindi_test_medical_genetics_test.csv
â”‚Â Â  â”‚Â Â  â””â”€â”€ mmlu_Hindi_test_professional_medicine_test.csv
â”‚Â Â  â”œâ”€â”€ spanish_headqa
â”‚Â Â  â”‚Â Â  â””â”€â”€ HEAD-QA-test.json
â”‚Â Â  â”œâ”€â”€ spanish_medexpqa
â”‚Â Â  â”‚Â Â  â””â”€â”€ test.es.casimedicos.rag.jsonl
â”‚Â Â  â””â”€â”€ spanish_mmlu
â”‚Â Â      â”œâ”€â”€ mmlu_Spanish_test_anatomy_test.csv
â”‚Â Â      â”œâ”€â”€ mmlu_Spanish_test_clinical_knowledge_test.csv
â”‚Â Â      â”œâ”€â”€ mmlu_Spanish_test_college_biology_test.csv
â”‚Â Â      â”œâ”€â”€ mmlu_Spanish_test_college_medicine_test.csv
â”‚Â Â      â”œâ”€â”€ mmlu_Spanish_test_medical_genetics_test.csv
â”‚Â Â      â””â”€â”€ mmlu_Spanish_test_professional_medicine_test.csv
â””â”€â”€ utils
    â”œâ”€â”€ README.md
    â”œâ”€â”€ answer_utils.py
    â”œâ”€â”€ benchmark_utils.py
    â”œâ”€â”€ eval_chatgpt_utils.py
    â”œâ”€â”€ eval_large_utils.py
    â”œâ”€â”€ eval_small_utils.py
    â”œâ”€â”€ test_extraction_chinese.py
    â”œâ”€â”€ test_extraction_english.py
    â”œâ”€â”€ test_extraction_french.py
    â”œâ”€â”€ test_extraction_hindi.py
    â”œâ”€â”€ test_extraction_spanish.py
    â””â”€â”€ utils.py
```

# Citation
If you find our work useful in your research, please consider citing it in your publications. We provide a BibTeX entry below.

```bibtex
@article{jia2024medpodgpt,
  title   = {{MedPodGPT}: A Multilingual Audio-augmented Large Language Model for Medical Research and Education},
  author  = {Shuyue Jia, Subhrangshu Bit, Edward Searls, Lindsey A. Claus, Pengrui Fan, Varuna H. Jasodanand, Meagan V. Lauber, Divya Veerapaneni, William M. Wang, Rhoda Au, Vijaya B. Kolachalama},
  journal = {medRxiv}
  year    = {2024},
}
```

# Contact
**Core Contributor and Maintainer**: <br>
- [Shuyue Jia](https://github.com/SuperBruceJia)
- [Subhrangshu Bit](https://github.com/SubhrangshuBit)
- [Edward Searls](https://github.com/nsearls-bu)
- [Pengrui Fan](https://github.com/pengrui26)
- [William M. Wang](https://github.com/bomas7)

If you have any questions, please drop us an email at [brucejia@bu.edu](brucejia@bu.edu), [sbit@bu.edu](sbit@bu.edu), and [nsearls@bu.edu](nsearls@bu.edu).

# Contribution
We always welcome contributions to help make **MedPodGPT** Library better. 
If you would like to contribute, please submit a [pull request](https://github.com/vkola-lab/medpodgpt/pulls).

# Acknowledgement
The **MedPodGPT** Library is created and maintained by the Kolachalama Lab at Boston University.

<a href="https://www.bu.edu/"> <img width="250" src="https://raw.githubusercontent.com/SuperBruceJia/promptcraft/main/bu.png"></a>
