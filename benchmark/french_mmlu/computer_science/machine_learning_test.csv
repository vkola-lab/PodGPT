Déclaration 1| L'estimateur de régression linéaire a la plus petite variance parmi tous les estimateurs non biaisés. Déclaration 2| Les coefficients α attribués aux classificateurs assemblés par AdaBoost sont toujours non négatifs.,"Vrai, vrai","Faux, faux","Vrai, faux","Faux, vrai",D
"Déclaration 1| RoBERTa se pré-entraîne sur un corpus environ 10 fois plus grand que celui sur lequel BERT a été pré-entraîné. Déclaration 2| En 2018, les ResNeXts utilisaient généralement des fonctions d'activation tanh.","Vrai, Vrai","Faux, Faux","Vrai, Faux","Faux, Vrai",C
"Déclaration 1| Les Machines à Vecteurs de Support, comme les modèles de régression logistique, donnent une distribution de probabilité sur les étiquettes possibles pour un exemple d'entrée. Déclaration 2| Nous nous attendons à ce que les vecteurs de support restent les mêmes en général lorsque nous passons d'un noyau linéaire à des noyaux polynomiaux d'ordre supérieur.","Vrai, vrai","Faux, faux","Vrai, faux","Faux, vrai",B
"Un problème d'apprentissage machine doit traiter quatre attributs plus une classe. Les attributs ont chacun 3, 2, 2 et 2 valeurs possibles. La classe a 3 valeurs possibles. Combien y a-t-il d'exemples différents maximum possibles?",12,24,48,72,D
"En 2020, quelle architecture est la meilleure pour classer des images de haute résolution?",réseaux de convolution,réseaux graphiques,réseaux entièrement connectés,réseaux RBF,A
Déclaration 1| La vraisemblance des données augmentera toujours au fil des itérations successives de l'algorithme de maximisation de l'espérance. Déclaration 2| Un inconvénient de Q-learning est qu'il ne peut être utilisé que lorsque l'apprenant a une connaissance préalable de la façon dont ses actions affectent son environnement.,"Vrai, Vrai","Faux, Faux","Vrai, Faux","Faux, Vrai",B
Supposons que nous avons calculé le gradient de notre fonction de coût et l'avons stocké dans un vecteur g. Quel est le coût d'une mise à jour de descente de gradient étant donné le gradient?,O(D),O(N),O(ND),O(ND^2),A
"Énoncé 1 | Pour une variable continue aléatoire x et sa fonction de distribution de probabilité p(x), il est vrai que 0 ≤ p(x) ≤ 1 pour tout x. Énoncé 2 | L'arbre de décision est appris en minimisant le gain d'information.","Vrai, Vrai","Faux, Faux","Vrai, Faux","Faux, Vrai",B
Considérez le réseau bayésien ci-dessous. Combien de paramètres indépendants sont nécessaires pour ce réseau bayésien H -> U <- P <- W?,2,4,8,16,C
"Si le nombre d'exemples d'entraînement tend vers l'infini, le modèle entraîné sur ces données aura:",Une variance plus faible,Une variance plus élevée,La même variance,Aucune des réponses ci-dessus,A
Déclaration 1| L'ensemble de tous les rectangles dans le plan 2D (qui inclut des rectangles non alignés sur les axes) peut briser un ensemble de 5 points. Déclaration 2| La dimension VC du classificateur k-Nearest Neighbour lorsque k = 1 est infinie.,"Vrai, vrai","Faux, faux","Vrai, faux","Faux, vrai",A
Le terme _ désigne un modèle qui ne peut ni modéliser les données de formation ni généraliser à de nouvelles données.,bon ajustement,surchargement,sous-ajustement,tout ce qui précède,C
Déclaration 1| Le score F1 peut être particulièrement utile pour les ensembles de données avec un déséquilibre de classe élevé. Déclaration 2| L'aire sous la courbe ROC est l'une des principales métriques utilisées pour évaluer les détecteurs d'anomalies.,"Vrai, Vrai","Faux, Faux","Vrai, Faux","Faux, Vrai",A
"Déclaration 1| L'algorithme de rétropropagation apprend un réseau neuronal globalement optimal avec des couches cachées. Déclaration 2| La dimension VC d'une ligne devrait être au maximum de 2, car je peux trouver au moins un cas de 3 points qui ne peuvent être brisés par n'importe quelle ligne.","Vrai, vrai","Faux, faux","Vrai, faux","Faux, vrai",B
Qu'est-ce que l'entropie élevée signifie en termes de classification des partitions ?,Pur,Non pur,Utile,Inutile,B
"Déclaration 1| La Normalisation de Couche est utilisée dans le document d'origine ResNet, pas la Normalisation par Lot. Déclaration 2| Les DCGAN utilisent l'auto-attention pour stabiliser l'entraînement.","Vrai, Vrai","Faux, Faux","Vrai, Faux","Faux, Vrai",B
"En construisant un modèle de régression linéaire pour un ensemble de données particulier, vous observez que le coefficient d'une des caractéristiques a une valeur négative relativement élevée. Cela suggère que",Cette caractéristique a un fort effet sur le modèle (doit être conservée),Cette caractéristique n'a pas un fort effet sur le modèle (doit être ignorée),Il n'est pas possible de commenter l'importance de cette caractéristique sans informations supplémentaires,Rien ne peut être déterminé.,C
"Pour un réseau de neurones, quelle est l'hypothèse structurelle qui affecte le plus le compromis entre un sous-apprentissage (c'est-à-dire un modèle à biais élevé) et un sur-apprentissage (c'est-à-dire un modèle à variance élevée) :",Le nombre de nœuds cachés,Le taux d'apprentissage,Le choix initial des poids,L'utilisation d'une entrée avec unité constante,A
"Pour la régression polynomiale, quelle est la principale hypothèse structurelle qui affecte le plus le compromis entre sous-apprentissage et surapprentissage?",Le degré du polynôme,Savoir si nous apprenons les poids par inversion de matrice ou descente de gradient,La variance supposée du bruit gaussien,L'utilisation d'une entrée constante-term unit,A
"Déclaration 1| En 2020, certains modèles ont une précision supérieure à 98% sur CIFAR-10. Déclaration 2| Les ResNets originaux n'ont pas été optimisés avec l'optimiseur Adam.","Vrai, Vrai","Faux, Faux","Vrai, Faux","Faux, Vrai",A
L'algorithme K-means:,Exige que la dimension de l'espace de caractéristiques ne soit pas supérieure au nombre d'échantillons,Présente la plus petite valeur de la fonction objective lorsque K = 1,Minimise la variance intra-classe pour un nombre donné de clusters,Converge vers l'optimum global si et seulement si les moyennes initiales sont choisies parmi les échantillons eux-mêmes,C
Déclaration 1| Les réseaux VGG ont des noyaux de convolution de largeur et de hauteur plus petits que ceux de la première couche d'AlexNet. Déclaration 2| Les procédures d'initialisation de poids dépendantes des données ont été introduites avant la normalisation par lots.,"Vrai, vrai","Faux, faux","Vrai, faux","Faux, vrai",A
"Quel est le rang de la matrice suivante ? A = [[1, 1, 1], [1, 1, 1], [1, 1, 1]]",0,1,2,3,B
"Statement 1| L'estimation de la densité (par exemple, en utilisant l'estimateur de densité de noyau) peut être utilisée pour effectuer une classification. Statement 2| La correspondance entre la régression logistique et le Bayes naïf gaussien (avec des covariances de classe identité) signifie qu'il y a une correspondance bijective entre les paramètres des deux classifieurs.","Vrai, vrai","Faux, faux","Vrai, faux","Faux, vrai",C
Supposons que nous voulions effectuer un regroupement de données spatiales telles que les emplacements géométriques des maisons. Nous souhaitons produire des groupes de tailles et de formes différentes. Lequel des méthodes suivantes convient le mieux?,Arbres de décision,Regroupement basé sur la densité,Regroupement basé sur le modèle,Regroupement K-means,B
"En AdaBoost, les poids des exemples mal classés augmentent-ils d'un même facteur multiplicatif ? En AdaBoost, l'erreur d'entraînement pondérée e_t du t-ième classificateur faible sur les données d'entraînement avec des poids D_t a tendance à augmenter en fonction de t.","Vrai, Vrai","Faux, Faux","Vrai, Faux","Faux, Vrai",A
Les estimations MLE sont souvent indésirables car,elles sont biaisées,elles ont une grande variance,elles ne sont pas des estimateurs cohérents,Aucune des réponses ci-dessus,B
"La complexité de calcul de la descente de gradient est,",linéaire en D,linéaire en N,polynomiale en D,dépendante du nombre d'itérations,C
Moyenner la sortie de plusieurs arbres de décision aide à _.,Augmenter le biais,Diminuer le biais,Augmenter la variance,Diminuer la variance,D
Le modèle obtenu en appliquant une régression linéaire sur le sous-ensemble de fonctionnalités identifié peut différer du modèle obtenu à la fin du processus d'identification du sous-ensemble lors,Sélection du meilleur sous-ensemble,Sélection ascendante par étapes,Sélection ascendante progressivement,Tout ce qui précède,C
Les réseaux de neurones:,Optimisent une fonction objectif convexe,Ne peuvent être entraînés qu'avec la descente de gradient stochastique,Peuvent utiliser un mélange de différentes fonctions d'activation,Aucune des options mentionnées ci-dessus,C
"Si l'incidence d'une maladie D est d'environ 5 cas pour 100 personnes (c'est-à-dire P(D) = 0,05). Soit une variable aléatoire booléenne D qui signifie qu'un patient «a la maladie D» et une variable aléatoire booléenne TP qui signifie «test positif». Les tests de la maladie D sont connus pour être très précis dans le sens où la probabilité de tester positif lorsque vous avez la maladie est de 0,99 et la probabilité de tester négatif lorsque vous n'avez pas la maladie est de 0,97. Quelle est la probabilité a priori P(TP) de tester positif ?",0.0368,0.473,0.078,Aucune des options ci-dessus,C
"Déclaration 1| Après avoir été cartographiés dans l'espace de fonctionnalités Q grâce à une fonction de noyau à base radiale, 1-NN utilisant une distance euclidienne non pondérée peut être en mesure d'atteindre de meilleures performances de classification que dans l'espace d'origine (même si nous ne pouvons pas garantir cela). Déclaration 2| La dimension VC d'un Perceptron est plus petite que la dimension VC d'un SVM linéaire simple.","Vrai, Vrai","Faux, Faux","Vrai, Faux","Faux, Vrai",B
Quel est le désavantage de Grid search ?,Il ne peut pas être appliqué aux fonctions non différenciables.,Il ne peut pas être appliqué aux fonctions non continues.,Il est difficile à implémenter.,Il s'exécute raisonnablement lentement pour la régression linéaire multiple.,D
Prédire la quantité de pluie dans une région sur la base de divers indices est un problème de ______.,l'apprentissage supervisé,l'apprentissage non supervisé,la classification en grappes,Aucune des réponses ci-dessus,A
Laquelle des phrases suivantes est fausse en ce qui concerne la régression?,Elle relie les entrées aux sorties.,Elle est utilisée pour la prédiction.,Elle peut être utilisée pour l'interprétation.,Elle découvre des relations causales.,D
Quelle est la principale raison de l'élagage d'un arbre de décision?,Pour économiser du temps de calcul lors des tests,Pour économiser de l'espace pour stocker l'arbre de décision,Pour réduire l'erreur de l'ensemble d'entraînement,Pour éviter un surapprentissage de l'ensemble d'entraînement,D
Déclaration 1| L'estimateur de densité noyau est équivalent à effectuer une régression noyau avec la valeur Yi = 1/n à chaque point Xi dans l'ensemble de données d'origine. Déclaration 2| La profondeur d'un arbre de décision appris peut être supérieure au nombre d'exemples d'entraînement utilisés pour créer l'arbre.,"Vrai, Vrai","Faux, Faux","Vrai, Faux","Faux, Vrai",B
Supposez que votre modèle souffre d'un surapprentissage. Laquelle des options suivantes n'est PAS une méthode valable pour essayer de réduire le surapprentissage?,Augmenter la quantité de données d'entraînement.,Améliorer l'algorithme d'optimisation utilisé pour minimiser l'erreur.,Réduire la complexité du modèle.,Réduire le bruit dans les données d'entraînement.,B
Déclaration 1| La fonction softmax est couramment utilisée en régression logistique mutliclasse. Déclaration 2| La température d'une distribution softmax non uniforme affecte son entropie.,"Vrai, Vrai","Faux, Faux","Vrai, Faux","Faux, Vrai",A
Quel(s) sont les éléments vrais concernant un SVM (Support Vector Machine) ?,"Pour des points de données à deux dimensions, le plan hyperplan séparé appris par un SVM linéaire sera une ligne droite.","En théorie, un SVM à noyau gaussien ne peut pas modéliser un plan hyperplan séparé complexe.","Pour chaque fonction de noyau utilisée dans un SVM, on peut obtenir une expansion de base fermée équivalente.",Le surajustement dans un SVM n'est pas une fonction du nombre de vecteurs de support.,A
"Quelle est la probabilité conjointe de H, U, P et W décrite par le réseau bayésien H -> U <- P <- W? [note : comme le produit des probabilités conditionnelles]","P(H, U, P, W) = P(H) * P(W) * P(P) * P(U)","P(H, U, P, W) = P(H) * P(W) * P(P | W) * P(W | H, P)","P(H, U, P, W) = P(H) * P(W) * P(P | W) * P(U | H, P)",Aucune des réponses précédentes,C
"Déclaration 1| Étant donné que la dimension VC d'un SVM avec un noyau radial est infinie, un tel SVM doit être pire qu'un SVM avec un noyau polynomial qui a une dimension VC finie. Déclaration 2| Un réseau de neurones à deux couches avec des fonctions d'activation linéaires est essentiellement une combinaison pondérée de séparateurs linéaires, formés sur un ensemble de données donné; l'algorithme de renforcement construit sur des séparateurs linéaires trouve également une combinaison de séparateurs linéaires, ces deux algorithmes donneront donc le même résultat.","Vrai, Vrai","Faux, Faux","Vrai, Faux","Faux, Vrai",B
Statement 1| L'algorithme ID3 garantit la découverte de l'arbre de décision optimal. Statement 2| Considérez une distribution de probabilité continue avec une densité f() qui est non nulle partout. La probabilité d'une valeur x est égale à f(x).,"Vrai, vrai","Faux, faux","Vrai, faux","Faux, vrai",B
"Étant donné un Réseau de Neurones avec N nœuds d'entrée, sans couches cachées, un nœud de sortie, avec perte d'entropie et fonctions d'activation Sigmoid, lequel des algorithmes suivants (avec les hyper-paramètres et l'initialisation appropriés) peut être utilisé pour trouver l'optimum global ?",La descente de gradient stochastique,La descente de gradient par mini-lots,La descente de gradient en batch,Tous les précédents,D
"Ajout de plus de fonctions de base dans un modèle linéaire, choisissez l'option la plus probable :",Diminue le biais du modèle,Diminue le biais de l'estimation,Diminue la variance,N'affecte ni le biais ni la variance,A
Considérez le réseau bayésien ci-dessous. Combien de paramètres indépendants aurions-nous besoin si nous ne faisions aucune hypothèse sur l'indépendance ou l'indépendance conditionnelle H -> U <- P <- W ?,3,4,7,15,D
Un autre terme pour la détection hors distribution est?,détection d'anomalie,détection à une classe,robustesse à la disparité d'entraînement-test,détection de fond,A
"Déclaration 1| Nous apprenons un classifieur f en renforçant des apprenants faibles h. La forme fonctionnelle de la frontière de décision de f est la même que celle de h, mais avec des paramètres différents. (par exemple, si h était un classifieur linéaire, alors f est également un classifieur linéaire). Déclaration 2| La validation croisée peut être utilisée pour sélectionner le nombre d'itérations dans le renforcement; cette procédure peut aider à réduire le surajustement.","Vrai, Vrai","Faux, Faux","Vrai, Faux","Faux, Vrai",D
Déclaration 1| Les réseaux d'autoroutes ont été introduits après les ResNets et évitent le max pooling au profit des convolutions. Déclaration 2| DenseNets coûtent généralement plus de mémoire que les ResNets.,"Vrai, Vrai","Faux, Faux","Vrai, Faux","Faux, Vrai",D
"Si N est le nombre d'instances dans l'ensemble de données d'entraînement, le temps d'exécution de la classification des plus proches voisins est de",O(1),O( N ),O(log N ),O( N^2 ),B
"Déclaration 1| Les ResNets et les Transformers originaux sont des réseaux de neurones à propagation avant. Déclaration 2| Les Transformers originaux utilisent l'auto-attention, mais l'original ResNet ne le fait pas.","Vrai, vrai","Faux, faux","Vrai, faux","Faux, vrai",A
"Déclaration 1 | Les RELUs ne sont pas monotones, mais les sigmoïdes sont monotones. Déclaration 2 | Les réseaux de neurones entraînés avec une descente de gradient convergent avec une forte probabilité vers l'optimum global.","Vrai, vrai","Faux, faux","Vrai, faux","Faux, vrai",D
La sortie numérique d'un nœud sigmoid dans un réseau neuronal :,"N'est pas bornée, englobant tous les nombres réels.","N'est pas bornée, englobant tous les entiers.",Est bornée entre 0 et 1.,Est bornée entre -1 et 1.,C
Quelle méthode suivante ne peut être utilisée que lorsque les données d'entraînement sont linéairement séparables?,SVM à marge dure linéaire.,Régression logistique linéaire.,SVM à marge douce linéaire.,La méthode du centroïde.,A
Quels sont les algorithmes de regroupement spatial suivants ?,Regroupement basé sur la partition,Regroupement par K-moyennes,Regroupement basé sur la grille,Tous les éléments ci-dessus,D
Déclaration 1| Les frontières de décision à marge maximale que construisent les machines à vecteurs de support ont l'erreur de généralisation la plus faible parmi tous les classificateurs linéaires. Déclaration 2| Toute frontière de décision que nous obtenons à partir d'un modèle génératif avec des distributions gaussiennes conditionnelles de classe pourrait en principe être reproduite avec un SVM et un noyau polynômial de degré inférieur ou égal à trois.,"Vrai, Vrai","Faux, Faux","Vrai, Faux","Faux, Vrai",D
Statement 1| La régularisation L2 des modèles linéaires a tendance à rendre les modèles plus clairsemés que la régularisation L1. Statement 2| Des connexions résiduelles peuvent être trouvées dans les ResNets et les Transformers.,"Vrai, vrai","Faux, faux","Vrai, faux","Faux, vrai",D
"Supposons que nous voulions calculer P(H|E, F) et que nous n'ayons pas d'informations sur l'indépendance conditionnelle. Parmi les ensembles de chiffres suivants, lequel est suffisant pour le calcul?","P(E, F), P(H), P(E|H), P(F|H)","P(E, F), P(H), P(E, F|H)","P(H), P(E|H), P(F|H)","P(E, F), P(E|H), P(F|H)",B
Quel parmi les éléments suivants empêche le surajustement lorsqu'on effectue le bagging?,L'utilisation de l'échantillonnage avec remplacement comme technique d'échantillonnage,L'utilisation de classifieurs faibles,L'utilisation d'algorithmes de classification qui ne sont pas sujets au surajustement,La pratique de validation effectuée sur chaque classifieur entraîné,B
"Déclaration 1 | PCA et Spectral Clustering (comme celui d'Andrew Ng) effectuent une décomposition en valeurs propres sur deux matrices différentes. Cependant, la taille de ces deux matrices est la même. Déclaration 2 | Puisque la classification est un cas particulier de la régression, la régression logistique est un cas particulier de la régression linéaire.","Vrai, Vrai","Faux, Faux","Vrai, Faux","Faux, Vrai",B
"Déclaration 1| Le Stanford Sentiment Treebank contenait des critiques de films, pas de livres. Déclaration 2| Le Penn Treebank a été utilisé pour la modélisation linguistique.","Vrai, vrai","Faux, faux","Vrai, faux","Faux, vrai",A
"Quelle est la dimension de l'espace nul de la matrice suivante? A = [[3, 2, −9], [−6, −4, 18], [12, 8, −36]]",0,1,2,3,C
Qu'est-ce que des vecteurs de support ?,Les exemples les plus éloignés de la frontière de décision.,Les seuls exemples nécessaires pour calculer f(x) dans un SVM.,Le centre de données.,Tous les exemples ayant un poids αk non nul dans un SVM.,B
Déclaration 1| Les paramètres Word2Vec n'ont pas été initialisés à l'aide d'une machine de Boltzmann restreinte. Déclaration 2| La fonction tanh est une fonction d'activation non linéaire.,"Vrai, Vrai","Faux, Faux","Vrai, Faux","Faux, Vrai",A
"Si votre perte de formation augmente avec le nombre d'époques, quel pourrait être un problème possible avec le processus d'apprentissage ?",La régularisation est trop faible et le modèle est surajusté,La régularisation est trop élevée et le modèle est sous-ajusté,La taille de pas est trop grande,La taille de pas est trop petite,C
"Supposons qu'un taux d'incidence de 5 cas pour 100 personnes (soit P(D) = 0,05) est enregistré pour une maladie D. Soit la variable aléatoire booléenne D définissant un patient ""ayant la maladie D"" et la variable aléatoire booléenne TP correspondant au résultat ""test positif"". Les tests pour la maladie D sont connus pour être très précis dans le sens où la probabilité d'obtenir un résultat positif lorsque l'on a la maladie est de 0,99 et la probabilité d'obtenir un résultat négatif lorsque l'on ne possède pas la maladie est de 0,97. Quelle est P(D | TP), c'est-à-dire la probabilité postérieure que vous ayez la maladie D lorsque le test est positif ?","0,0495","0,078","0,635","0,97",C
"Les résultats de l'apprentissage automatique traditionnel supposent que les ensembles d'entraînement et de test sont indépendants et identiquement distribués. En 2017, les modèles COCO étaient généralement pré-entraînés sur ImageNet.","Vrai, vrai","Faux, faux","Vrai, faux","Faux, vrai",A
"Déclaration 1| Les valeurs des marges obtenues par deux noyaux différents K1 (x, x0) et K2 (x, x0) sur le même ensemble d'entraînement ne nous indiquent pas quel classifieur fonctionnera mieux sur l'ensemble de test. Déclaration 2| La fonction d'activation de BERT est GELU.","Vrai, vrai","Faux, faux","Vrai, faux","Faux, vrai",A
Quel est l'algorithme de clustering en apprentissage automatique?,Expectation Maximization,CART,Gaussian Naïve Bayes,Apriori,A
"Vous venez de terminer l'entraînement d'un arbre de décision pour la classification du spam, mais ses performances sont anormalement mauvaises sur vos ensembles d'entraînement et de test. Vous savez que votre mise en œuvre n'a pas de bugs, alors qu'est-ce qui pourrait causer le problème?",Vos arbres de décision sont trop peu profonds.,Vous devez augmenter le taux d'apprentissage.,Vous faites du surapprentissage.,Aucun des éléments ci-dessus.,A
La validation croisée K-fold est,linéaire en K,quadratique en K,cubique en K,exponentielle en K,A
"Déclaration 1| Les réseaux de neurones à grande échelle sont normalement entraînés sur des processeurs, pas des cartes graphiques. Déclaration 2| Le modèle ResNet-50 comporte plus d'un milliard de paramètres.","Vrai, Vrai","Faux, Faux","Vrai, Faux","Faux, Vrai",B
"Étant donné deux variables aléatoires booléennes, A et B, où P(A) = 1/2, P(B) = 1/3 et P(A | ¬B) = 1/4, quelle est la valeur de P(A | B) ?",1/6,1/4,3/4,1,D
Les risques existentiels posés par l'IA sont le plus couramment associés à quel professeur parmi les suivants?,Nando de Frietas,Yann LeCun,Stuart Russell,Jitendra Malik,C
Statement 1| Maximiser la vraisemblance du modèle de régression logistique génère plusieurs optima locaux. Statement 2| Aucun classificateur ne peut faire mieux qu'un classificateur Bayésien naïf si la distribution des données est connue.,"Vrai, vrai","Faux, faux","Vrai, faux","Faux, vrai",B
"Pour la régression de noyau, quelle est l'une de ces hypothèses structurelles qui affecte le plus le compromis entre le sous-ajustement et le surajustement :","Si la fonction de noyau est gaussienne, triangulaire ou en forme de boîte","Si nous utilisons les métriques Euclidienne, L1 ou L∞",La largeur du noyau,La hauteur maximale de la fonction de noyau,C
"Déclaration 1| L'algorithme d'apprentissage SVM est garanti pour trouver l'hypothèse optimale globale par rapport à sa fonction d'objet. Déclaration 2| Après avoir été cartographié dans l'espace de fonction Q via une fonction de noyau à base radiale, un perceptron peut être en mesure d'obtenir de meilleures performances de classification que dans son espace d'origine (bien que nous ne puissions pas garantir cela).","Vrai, Vrai","Faux, Faux","Vrai, Faux","Faux, Vrai",A
"Pour un classificateur Bayésien gaussien, quelle est l'assomption structurelle qui affecte le plus le compromis entre le sous-ajustement et le sur-ajustement:",Si nous apprenons les centres de classe par maximum de vraisemblance ou par descente de gradient,Si nous supposons des matrices de covariance de classe complètes ou diagonales,Si nous avons des priors de classe égales ou des priors estimées à partir des données.,Si nous autorisons les classes à avoir des vecteurs de moyenne différents ou si nous les forçons à partager le même vecteur de moyenne,B
Déclaration 1| L'overfitting est plus probable lorsque l'ensemble de données d'entraînement est petit. Déclaration 2| L'overfitting est plus probable lorsque l'espace d'hypothèses est petit.,"Vrai, vrai","Faux, faux","Vrai, faux","Faux, vrai",D
"Statement 1 | En plus de EM, la descente de gradient peut être utilisée pour effectuer une inférence ou un apprentissage sur un modèle de mélange gaussien. Déclaration 2 | En supposant un nombre fixe d'attributs, un classificateur optimal de Bayes basé sur les Gaussiennes peut être appris en temps linéaire par rapport au nombre d'enregistrements dans l'ensemble de données.","Vrai, vrai","Faux, faux","Vrai, faux","Faux, vrai",A
"En réseau bayésien, les résultats de l'inférence de l'algorithme de junction tree sont identiques aux résultats de l'élimination de variable. Si deux variables aléatoires X et Y sont conditionnellement indépendantes de la variable aléatoire Z, alors dans le réseau bayésien correspondant, les nœuds pour X et Y sont d-séparés étant donné Z.","Vrai, Vrai","Faux, Faux","Vrai, Faux","Faux, Vrai",C
"Étant donné un grand ensemble de données médicales de patients souffrant de maladies cardiaques, essayez de déterminer s'il peut y avoir différents groupes de tels patients pour lesquels nous pourrions adapter des traitements séparés. Quel type de problème d'apprentissage est-ce ?",Apprentissage supervisé,Apprentissage non supervisé,Les deux (a) et (b),Ni (a) ni (b),B
Que feriez-vous dans PCA pour obtenir la même projection que SVD?,Transformer les données en moyenne zéro,Transformer les données en médiane zéro,Pas possible,Aucun de ceux-ci,A
"Déclaration 1| L'erreur d'entraînement du classificateur de plus proche voisin à 1 est de 0. Déclaration 2| À mesure que le nombre de points de données augmente jusqu'à l'infini, l'estimation MAP se rapproche de l'estimation MLE pour tous les priors possibles. En d'autres termes, avec suffisamment de données, le choix de prior est sans importance.","Vrai, vrai","Faux, faux","Vrai, faux","Faux, vrai",C
"En effectuant une régression des moindres carrés avec régularisation (en supposant que l'optimisation peut être effectuée exactement), l'augmentation de la valeur du paramètre de régularisation λ :",ne diminuera jamais l'erreur d'entraînement.,ne augmentera jamais l'erreur d'entraînement.,ne diminuera jamais l'erreur de test.,ne augmentera jamais l'erreur de test.,A
Quelle est la meilleure description de ce que les approches discriminatoires essaient de modéliser ? (w sont les paramètres dans le modèle),"p(y|x, w)","p(y, x)","p(w|x, w)",Aucune des réponses ci-dessus,A
Déclaration 1| La performance de classification CIFAR-10 pour les réseaux de neurones convolutifs peut dépasser 95%. Déclaration 2| Les ensembles de réseaux de neurones n'améliorent pas la précision de classification car les représentations qu'ils apprennent sont fortement corrélées.,"Vrai, Vrai","Faux, Faux","Vrai, Faux","Faux, Vrai",C
Sur lequel des points suivants les bayésiens et les fréquentistes seraient-ils en désaccord?,L'utilisation d'un modèle de bruit non gaussien dans la régression probabiliste.,L'utilisation de la modélisation probabiliste pour la régression.,L'utilisation de distributions a priori sur les paramètres dans un modèle probabiliste.,L'utilisation de classes a priori dans l'analyse discriminante gaussienne.,C
"Déclaration 1| La métrique BLEU utilise la précision, tandis que la métrique ROGUE utilise le rappel. Déclaration 2| Les modèles de Markov cachés étaient fréquemment utilisés pour modéliser les phrases anglaises.","Vrai, Vrai","Faux, Faux","Vrai, Faux","Faux, Vrai",A
Déclaration 1| ImageNet contient des images de résolutions différentes. Déclaration 2| Caltech-101 a plus d'images que ImageNet.,"Vrai, Vrai","Faux, Faux","Vrai, Faux","Faux, Vrai",C
Quel est le plus approprié pour la sélection de fonctionnalités?,Ridge,Lasso,les deux (a) et (b),ni (a) ni (b),B
Supposez que vous disposez d'un algorithme EM qui trouve les estimations du maximum de vraisemblance pour un modèle avec des variables latentes. On vous demande de modifier l'algorithme pour qu'il trouve plutôt les estimations MAP. Quelle étape ou quelles étapes devez-vous modifier ?,Expectation,Maximization,Aucune modification nécessaire,Les deux,B
"Pour un classifieur de Bayes gaussien, quelle est la supposition structurelle qui affecte le plus le compromis entre sous-apprentissage et sur-apprentissage :",Si nous apprenons les centres de classe par Maximum de vraisemblance ou descente de gradient,Si nous supposons des matrices de covariance de classe complètes ou diagonales,Si nous avons des priors de classe égaux ou des priors estimés à partir des données,Si nous autorisons les classes à avoir différents vecteurs moyens ou si nous les forçons à partager le même vecteur moyen,B
"Déclaration 1| Pour deux variables x et y ayant une distribution conjointe p(x, y), nous avons toujours H[x, y] ≥ H[x] + H[y] où H est la fonction d'entropie. Déclaration 2| Pour certains graphes dirigés, la moralisation diminue le nombre d'arêtes présentes dans le graphe.","Vrai, vrai","Faux, faux","Vrai, faux","Faux, vrai",B
Lequel des éléments suivants n'est PAS un apprentissage supervisé?,PCA,Arbre de décision,Régression linéaire,Bayésien naïf,A
Déclaration 1| La convergence d'un réseau neuronal dépend du taux d'apprentissage. Déclaration 2| Dropout multiplie les valeurs d'activation choisies au hasard par zéro.,"Vrai, vrai","Faux, faux","Vrai, faux","Faux, vrai",A
"Quelle est l'équation de P(A, B, C), étant donné les variables aléatoires booléennes A, B et C, et aucune hypothèse d'indépendance ou de conditionnement d'indépendance entre eux?",P(A | B) * P(B | C) * P(C | A),"P(C | A, B) * P(A) * P(B)","P(A, B | C) * P(C)","P(A | B, C) * P(B | A, C) * P(C | A, B)",C
Quelle est la meilleure tâche pour laquelle on peut utiliser le clustering ?,Prévoir la quantité de pluie en fonction de diverses indices,Détecter les transactions frauduleuses par carte de crédit,Entraîner un robot à résoudre un labyrinthe,Toutes les réponses précédentes,B
"Après l'application d'une pénalité de régularisation dans la régression linéaire, vous constatez que certains des coefficients de w ont été mis à zéro. Quelle pénalité suivante pourrait avoir été utilisée ?",Norme L0,Norme L1,Norme L2,(a) ou (b),D
"A et B sont deux événements. Si P(A, B) diminue tandis que P(A) augmente, lequel des éléments suivants est vrai?",P(A|B) diminue,P(B|A) diminue,P(B) diminue,Tout ce qui précède,B
"Déclaration 1| Lors de l'apprentissage d'un HMM pour un ensemble fixe d'observations, supposons que nous ne connaissons pas le nombre réel d'états cachés (ce qui est souvent le cas), nous pouvons toujours augmenter la vraisemblance des données d'entraînement en permettant plus d'états cachés. Déclaration 2| La filtrage collaboratif est souvent un modèle utile pour modéliser les préférences cinématographiques des utilisateurs.","Vrai, vrai","Faux, faux","Vrai, faux","Faux, vrai",A
"Vous entraînez un modèle de régression linéaire pour une tâche d'estimation simple et remarquez que le modèle surajuste les données. Vous décidez d'ajouter une régularisation $\ell_2$ pour pénaliser les poids. En augmentant le coefficient de régularisation $\ell_2$, que se passera-t-il avec le biais et la variance du modèle?",Biais augmentera ; Variance augmentera,Biais augmentera ; Variance diminuera,Biais diminuera ; Variance augmentera,Biais diminuera ; Variance diminuera,B
"Quelle(s) commande(s) PyTorch 1.8 produisent une matrice Gaussienne de dimension $10\times 5$ avec chaque entrée i.i.d. échantillonnée à partir de $\mathcal{N}(\mu=5,\sigma^2=16)$ et une matrice uniforme de dimension $10\times 10$ avec chaque entrée i.i.d. échantillonnée à partir de $U[-1,1)$?","\texttt{5 + torch.randn(10,5) * 16} ; \texttt{torch.rand(10,10,low=-1,high=1)}","\texttt{5 + torch.randn(10,5) * 16} ; \texttt{(torch.rand(10,10) - 0.5) / 0.5}","\texttt{5 + torch.randn(10,5) * 4} ; \texttt{2 * torch.rand(10,10) - 1}","\texttt{torch.normal(torch.ones(10,5)*5,torch.ones(5,5)*16)} ; \texttt{2 * torch.rand(10,10) - 1}",C
"Énoncé 1| Le gradient de ReLU est nul pour $x<0$, et le gradient sigmoid $\sigma(x)(1-\sigma(x))\le \frac{1}{4}$ pour tout $x$. Énoncé 2| Le sigmoid a un gradient continu et ReLU a un gradient discontinu.","Vrai, vrai","Faux, faux","Vrai, faux","Faux, vrai",A
Qu'est-ce qui est vrai à propos de la Normalisation en Batch ?,"Après l'application de la Normalisation en Batch, les activations du calque suivront une distribution gaussienne standard.",Le paramètre de biais des calques affines devient redondant si une couche de Normalisation en Batch suit immédiatement après.,L'initialisation standard des poids doit être modifiée lors de l'utilisation de la Normalisation en Batch.,La Normalisation en Batch est équivalente à la Normalisation de Couche pour les réseaux de neurones convolutifs.,B
Supposons que nous avons la fonction objectif suivante: $\argmin_{w} \frac{1}{2} \norm{Xw-y}^2_2 + \frac{1}{2}\gamma \norm{w}^2_2$. Quel est le gradient de $\frac{1}{2} \norm{Xw-y}^2_2 + \frac{1}{2}\lambda \norm{w}^2_2$ par rapport à $w$?,$\nabla_w f(w) = (X^\top X + \lambda I)w - X^\top y + \lambda w$,$\nabla_w f(w) = X^\top X w - X^\top y + \lambda$,$\nabla_w f(w) = X^\top X w - X^\top y + \lambda w$,$\nabla_w f(w) = X^\top X w - X^\top y + (\lambda+1) w$,C
Quelle est la caractéristique vraie d'un noyau de convolution ?,Convolutionner une image avec $\begin{bmatrix}1 & 0 & 0\\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix}$ ne changerait pas l'image,Convolutionner une image avec $\begin{bmatrix}0 & 0 & 0\\ 0 & 1 & 0 \\ 0 & 0 & 0 \end{bmatrix}$ ne changerait pas l'image,Convolutionner une image avec $\begin{bmatrix}1 & 1 & 1\\ 1 & 1 & 1 \\ 1 & 1 & 1 \end{bmatrix}$ ne changerait pas l'image,Convolutionner une image avec $\begin{bmatrix}0 & 0 & 0\\ 0 & 0 & 0 \\ 0 & 0 & 0 \end{bmatrix}$ ne changerait pas l'image,B
Lequel des énoncés suivants est faux?,"Les modèles de segmentation sémantique prédisent la classe de chaque pixel, tandis que les classificateurs d'images multiclasse prédisent la classe de l'image entière.",Une boîte englobante avec un IoU (intersection sur union) égal à 96% serait susceptible d'être considérée comme un vrai positif.,"Quand une boîte englobante prédite ne correspond à aucun objet dans la scène, elle est considérée comme un faux positif.",Une boîte englobante avec un IoU (intersection sur union) égal à 3% serait susceptible d'être considérée comme un faux négatif.,D
Quelle affirmation est fausse ?,"Le réseau entièrement connecté suivant sans fonctions d'activation est linéaire : $g_3(g_2(g_1(x)))$, où $g_i(x) = W_i x$ et $W_i$ sont des matrices.","Leaky ReLU $\max\{0,01x,x\}$ est convexe.",Une combinaison de ReLUs comme $ReLU(x) - ReLU(x-1)$ est convexe.,La perte $\log \sigma(x)= -\log(1+e^{-x})$ est concave,C
"Nous entraînons un réseau entièrement connecté doté de deux couches cachées pour prédire les prix des logements. Les entrées sont de dimensions $100$, et ont plusieurs caractéristiques telles que le nombre de pieds carrés, le revenu médian des familles, etc. La première couche cachée a $1000$ activations. La deuxième couche cachée a $10$ activations. La sortie est un scalaire représentant le prix de la maison. En supposant un réseau vanille avec des transformations affines et sans normalisation des lots ni de paramètres apprenables dans la fonction d'activation, combien de paramètres ce réseau possède-t-il ?",111021,110010,111110,110011,A
La dérivée du sigmoid $\sigma(x)=(1+e^{-x})^{-1}$ par rapport à $x$ est égale à $\text{Var}(B)$ où $B\sim \text{Bern}(\sigma(x))$ est une variable aléatoire de Bernoulli. Mettre les paramètres de biais dans chaque couche d'un réseau de neurones à 0 change le compromis biais-variance de telle sorte que la variance du modèle augmente et le biais du modèle diminue.,"Vrai, Vrai","Faux, Faux","Vrai, Faux","Faux, Vrai",C
